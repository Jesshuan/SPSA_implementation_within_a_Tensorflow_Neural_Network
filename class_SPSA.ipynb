{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST SPSA IMPLEMENTATION IN TENSORFLOW"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective : Test the SPSA alorithm method for a gradient descent of the loss function, within a complex model like a Convolution Neural Network.\n",
    "We use the Model keras class object in Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with HiddenPrints():\n",
    "    print(\"This will not be printed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_for_SPSA(tf.keras.Model):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super(Model_for_SPSA, self).__init__()\n",
    "\n",
    "        # Define the CONV model\n",
    "        self.inputs_shape = (28, 28, 1, )\n",
    "        self.conv_1 = tf.keras.layers.Conv2D(64, 2, input_shape=self.inputs_shape)\n",
    "        self.conv_2 = tf.keras.layers.Conv2D(16, 2)\n",
    "        self.conv_3 = tf.keras.layers.Conv2D(8, 2)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense_final = tf.keras.layers.Dense(10, \"softmax\")\n",
    "\n",
    "        # And to automaticly active the model :\n",
    "        data_activate = np.random.random(self.inputs_shape)\n",
    "        self(np.expand_dims(data_activate, axis=0))\n",
    "\n",
    "        self.metric_list = []\n",
    "        self.nb_layers = len(self.weights)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv_1(inputs)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.conv_3(x)\n",
    "        x = self.flatten(x)\n",
    "        return self.dense_final(x)\n",
    "    \n",
    "    def compile_metrics(self, metric_list): # a special method to compile any metric\n",
    "        for metric in metric_list:\n",
    "            self.metric_list.append(metric)\n",
    "\n",
    "    def compile_SPSA_parameters(self, c, alpha, gamma): # a special method to compile SPSA optimizer (hyperparameters)\n",
    "        self.spsa_alpha = alpha\n",
    "        self.spsa_c = c\n",
    "        self.spsa_gamma = gamma\n",
    "\n",
    "\n",
    "    def grad_loss_spsa(self, c_k, batch_data, batch_label): #to compute the gradient of loss function with the SPSA algorithm\n",
    "\n",
    "        with HiddenPrints():\n",
    "                    model_plus = copy.copy(self)\n",
    "                    model_minus = copy.copy(self)\n",
    "\n",
    "        Delta_list = [] # we have to stock the Delta vectors for each layer\n",
    "\n",
    "        for i in range(self.nb_layers):\n",
    "\n",
    "            dim_base_weight = np.shape(np.array(self.trainable_weights[i])) # we take the dimension of the keras layer\n",
    "\n",
    "            D = c_k*(2*np.round(np.random.random(dim_base_weight)) - 1) # we compute the random vector perturbation\n",
    "            \n",
    "            # Two model randomly perturbed with a gap = 2 * Delta*c_k:\n",
    "            model_plus.trainable_weights[i].assign_add(model_plus.trainable_weights[i] + D) \n",
    "            model_minus.trainable_weights[i].assign_add(model_minus.trainable_weights[i] - D)\n",
    "            \n",
    "            # we stock Delta vector for compute the final update for each layer later\n",
    "            Delta_list.append(D)\n",
    "\n",
    "        # return the list of Delta vector, and the global approximation SPSA derivate loss scalar\n",
    "        return Delta_list , (self.loss(batch_label, model_plus(batch_data)) - self.loss(batch_label, model_minus(batch_data))) / (2*c_k)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def fit_SPSA(self, data_train, validation_data, epochs): # special method of fit with SPSA method\n",
    "        # data_train : a dataset object\n",
    "        # validation_data : a dataset object\n",
    "\n",
    "        # --- Define Variables ---\n",
    "        \n",
    "        #Computing the targets of the train and test dataset\n",
    "        labels_train = np.concatenate([y for x, y in data_train], axis=0)\n",
    "        labels_test = np.concatenate([y for x, y in validation_data], axis=0)\n",
    "\n",
    "        # we also have to find our batch_size\n",
    "\n",
    "        for data in data_train.take(1):\n",
    "            batch_size = np.shape(data[0])[0]\n",
    "\n",
    "        steps_per_epochs = len(data_train)\n",
    "        steps_per_epochs_test = len(validation_data)\n",
    "\n",
    "        #--- Define SPSA additionnal parameters ---\n",
    "\n",
    "        # to define the magnitude_g0, we take all the dataset divised by 10... (not just a little batch)... to have a better approximation\n",
    "        batch_data_big_sample = next(iter(data_train.rebatch(int(steps_per_epochs * batch_size / 10))))[0]\n",
    "        batch_label_big_sample = next(iter(data_train.rebatch(int(steps_per_epochs * batch_size / 10))))[1]\n",
    "\n",
    "        _ , derivate_loss_init = self.grad_loss_spsa(self.spsa_c, batch_data_big_sample, batch_label_big_sample)\n",
    "\n",
    "        magnitude_g0 = np.abs(derivate_loss_init)\n",
    "\n",
    "        A = 0.1*epochs\n",
    "\n",
    "        a = 0.1*((A+1)**self.spsa_alpha)/magnitude_g0\n",
    "\n",
    "        print('Hyperparameters initialized for the beginning of the fit :')\n",
    "        print(f'- derivate_loss_init : {derivate_loss_init}')\n",
    "        print(f'- magnitude_g0 : {magnitude_g0}')\n",
    "        print(f'- a : {a}')\n",
    "        print(f'- c : {self.spsa_c}')\n",
    "\n",
    "        # We also need a big batch to compute the loss of the test set (for final results of each epoch)\n",
    "\n",
    "        batch_data_big_sample_test = next(iter(validation_data.rebatch(int(steps_per_epochs_test * batch_size / 10))))[0]\n",
    "        batch_label_big_sample_test = next(iter(validation_data.rebatch(int(steps_per_epochs_test * batch_size / 10))))[1]\n",
    "\n",
    "        # --- fit ----\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            print(f'--- epoch : {epoch} / {epochs} ---')\n",
    "\n",
    "            for step in range(steps_per_epochs):\n",
    "                clear_output(wait=True)\n",
    "                print(f'--- epoch : {epoch} / {epochs} ---')\n",
    "                print(f'step : {step} / {steps_per_epochs}')\n",
    "\n",
    "\n",
    "                # update a_k and c_k ---\n",
    "                a_k = a / (epoch + 1 + A)**self.spsa_alpha\n",
    "                c_k = self.spsa_c / (epoch + 1)**self.spsa_gamma\n",
    "\n",
    "                print(f'a_k : {a_k} - c_k : {c_k}')\n",
    "\n",
    "                # Compute the SPSA gradient of the loss function for a sample batch ---\n",
    "\n",
    "                batch_data = next(iter(data_train))[0]\n",
    "                batch_label = next(iter(data_train))[1]\n",
    "\n",
    "                Delta_list, derivate_loss = self.grad_loss_spsa(c_k, batch_data, batch_label)\n",
    "\n",
    "                # Update condition ---\n",
    "\n",
    "                update=True # variable of updating condition (transformed in \"False\" if one of all the layers become too big !...)\n",
    "\n",
    "                for i in range(self.nb_layers): # for each layer we compare the norm of the tensor of all the weights (divised by the number of weights) with our LIMIt parameter\n",
    "\n",
    "                    # Uncomment for a look on the vectors... and its norm...\n",
    "                    #print(f'for variation {i} : ')\n",
    "                    #print(f'norme variation vecteur poids à venir_{i} : {np.linalg.norm(- a_k * c_k * (derivate_loss / Delta_list[i]))}')\n",
    "                    #print(f'norme variation vecteur poids à venir (normé)_{i} : {np.linalg.norm(- a_k * c_k * (derivate_loss / Delta_list[i])) / np.product(np.shape(Delta_list[i]))}')\n",
    "                    #print(a_k * c_k * derivate_loss / Delta_list[i])\n",
    "                    if np.linalg.norm(- a_k * c_k * (derivate_loss / Delta_list[i])) / np.product(np.shape(Delta_list[i])) > LIMIT:\n",
    "                        #print(f'limits for {i}...no update')\n",
    "                        update=False # all the layers not will be update\n",
    "                        break\n",
    "\n",
    "                if update:\n",
    "                    for i in range(self.nb_layers):\n",
    "                        self.trainable_weights[i].assign_add( self.trainable_weights[i] - a_k * c_k * (derivate_loss / Delta_list[i]) )\n",
    "\n",
    "\n",
    "                        #print(f'result vecteur poids_{i}')\n",
    "                        #print(self.trainable_weights[i])\n",
    "                        #print(f'result variation_{i} on norm : {np.linalg.norm(- a_k * c_k * (derivate_loss / Delta_list[i]))}')\n",
    "\n",
    "            # Display the results of the loss and metric at the end of the epoch :\n",
    "\n",
    "            print(f'results epochs {epoch + 1}/{epochs} : ')\n",
    "\n",
    "            print('--Train_set--')\n",
    "\n",
    "            print(f'Loss_function : {self.loss(batch_label_big_sample, self(batch_data_big_sample))}')\n",
    "            y_pred = self.predict(data_train)\n",
    "            self.metric_list[0].update_state(labels_train, y_pred)\n",
    "            print(f'metric : {self.metric_list[0].result().numpy()}')\n",
    "\n",
    "            print('--Test_set--')\n",
    "\n",
    "            print(f'Loss_function : {self.loss(batch_label_big_sample_test, self(batch_data_big_sample_test))}')\n",
    "\n",
    "            y_pred = self.predict(validation_data)\n",
    "            self.metric_list[0].update_state(labels_test, y_pred)\n",
    "            print(f'metric : {self.metric_list[0].result().numpy()}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT DATA (MNIST) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "ds_train = ds_train.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(BATCH_SIZE)\n",
    "ds_train = ds_train.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = ds_test.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.batch(BATCH_SIZE)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST WITH A COMMON FIT :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_common = Model_for_SPSA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_common.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                   loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                   metrics=tf.keras.metrics.SparseCategoricalAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "938/938 [==============================] - 37s 39ms/step - loss: 0.3591 - sparse_categorical_accuracy: 0.8971 - val_loss: 0.3028 - val_sparse_categorical_accuracy: 0.9142\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 36s 38ms/step - loss: 0.2976 - sparse_categorical_accuracy: 0.9172 - val_loss: 0.2904 - val_sparse_categorical_accuracy: 0.9208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc118b61ea0>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_common.fit(ds_train, validation_data=ds_test, epochs=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST WITH THE SPSA FIT :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPSA_model = Model_for_SPSA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPSA_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPSA_model.compile_metrics([tf.keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPSA_model.compile_SPSA_parameters(0.001, 0.01, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch : 1 / 2 ---\n",
      "step : 937 / 938\n",
      "a_k : 0.014062226559774724 - c_k : 0.000993092495437036\n",
      "results epochs 2/2 : \n",
      "--Train_set--\n",
      "Loss_function : nan\n",
      "938/938 [==============================] - 8s 8ms/step\n",
      "metric : 0.09866154193878174\n",
      "--Test_set--\n",
      "Loss_function : nan\n",
      "157/157 [==============================] - 1s 8ms/step\n",
      "metric : 0.09861428290605545\n"
     ]
    }
   ],
   "source": [
    "SPSA_model.fit_SPSA(ds_train, ds_test, epochs=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
